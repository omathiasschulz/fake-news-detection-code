


MELHORES MODELOS - RNA MLP - DATASET 50 PALAVRAS

MODELO 01
# Teste MLP - 2021-02-18 10:20:50.444755 - dataset_50_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 12, "activation": "relu"}, {"qtd_neurons": 8, "activation": "tanh"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.1339; accuracy_model(%): 95.6240; accuracy_detection(%): 96.3374; rmse: 0.0581; precision: 0.9631; recall: 0.9635; f1-score: 0.9633; confusion_matrix: 3456-143-108-3146;

MODELO 02
# Teste MLP - 2021-02-18 10:27:42.153625 - dataset_50_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 12, "activation": "elu"}, {"qtd_neurons": 8, "activation": "elu"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.1234; accuracy_model(%): 95.7861; accuracy_detection(%): 95.3159; rmse: 0.0693; precision: 0.9528; recall: 0.9534; f1-score: 0.9531; confusion_matrix: 3412-187-134-3120;

MODELO 03
# Teste MLP - 2021-02-18 10:23:06.825242 - dataset_50_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 12, "activation": "relu"}, {"qtd_neurons": 8, "activation": "elu"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.1241; accuracy_model(%): 95.6240; accuracy_detection(%): 96.2060; rmse: 0.0605; precision: 0.9617; recall: 0.9624; f1-score: 0.9620; confusion_matrix: 3439-160-100-3154;

MODELO 04
# Teste MLP - 2021-02-18 11:45:23.438448 - dataset_50_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 300, "activation": "relu"}, {"qtd_neurons": 128, "activation": "tanh"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.4219; accuracy_model(%): 94.9757; accuracy_detection(%): 98.1322; rmse: 0.0550; precision: 0.9812; recall: 0.9813; f1-score: 0.9813; confusion_matrix: 3533-66-62-3192;

MODELO 05
# Teste MLP - 2021-02-19 07:43:09.928320 - dataset_50_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 200, "activation": "elu"}, {"qtd_neurons": 300, "activation": "elu"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.1774; accuracy_model(%): 94.9757; accuracy_detection(%): 96.2644; rmse: 0.0608; precision: 0.9623; recall: 0.9631; f1-score: 0.9626; confusion_matrix: 3433-166-90-3164;

MODELO 06
# Teste MLP - 2021-02-19 08:35:47.029355 - dataset_50_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 75, "activation": "relu"}, {"qtd_neurons": 128, "activation": "elu"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.2686; accuracy_model(%): 95.6240; accuracy_detection(%): 98.0009; rmse: 0.0476; precision: 0.9800; recall: 0.9799; f1-score: 0.9800; confusion_matrix: 3533-66-71-3183;

MODELO 07
# Teste MLP - 2021-02-20 09:31:10.334016 - dataset_50_palavras.csv
{"epochs": 50, "batch_size": 2, "layers": [{"qtd_neurons": 300, "activation": "relu"}, {"qtd_neurons": 128, "activation": "tanh"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.2489; accuracy_model(%): 94.8136; accuracy_detection(%): 98.0884; rmse: 0.0568; precision: 0.9809; recall: 0.9808; f1-score: 0.9808; confusion_matrix: 3536-63-68-3186;

MODELO 08
# Teste MLP - 2021-02-20 09:38:47.421948 - dataset_50_palavras.csv
{"epochs": 50, "batch_size": 2, "layers": [{"qtd_neurons": 200, "activation": "elu"}, {"qtd_neurons": 300, "activation": "elu"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.2410; accuracy_model(%): 94.0032; accuracy_detection(%): 96.9356; rmse: 0.0631; precision: 0.9690; recall: 0.9699; f1-score: 0.9693; confusion_matrix: 3451-148-62-3192;

MODELO 09
# Teste MLP - 2021-02-20 09:48:54.356669 - dataset_50_palavras.csv
{"epochs": 50, "batch_size": 2, "layers": [{"qtd_neurons": 75, "activation": "relu"}, {"qtd_neurons": 128, "activation": "elu"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.3366; accuracy_model(%): 94.9757; accuracy_detection(%): 97.7966; rmse: 0.0528; precision: 0.9778; recall: 0.9781; f1-score: 0.9779; confusion_matrix: 3511-88-63-3191;

MELHOR MODELO - MODELO 07
# Teste MLP - 2021-02-20 09:31:10.334016 - dataset_50_palavras.csv
{"epochs": 50, "batch_size": 2, "layers": [{"qtd_neurons": 300, "activation": "relu"}, {"qtd_neurons": 128, "activation": "tanh"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.2489; accuracy_model(%): 94.8136; accuracy_detection(%): 98.0884; rmse: 0.0568; precision: 0.9809; recall: 0.9808; f1-score: 0.9808; confusion_matrix: 3536-63-68-3186;


MELHORES MODELOS - RNA MLP - DATASET 100 PALAVRAS

# MODELO MLP - TESTES COM FUNÇÕES DE ATIVAÇÃO VARIADAS - 100 PALAVRAS

# 1º
# Teste MLP - 2021-02-20 10:38:58.807306 - dataset_100_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 12, "activation": "relu"}, {"qtd_neurons": 8, "activation": "tanh"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.1483; accuracy_model(%): 95.0538; accuracy_detection(%): 95.4123; rmse: 0.0696; precision: 0.9402; recall: 0.9550; f1-score: 0.9471; confusion_matrix: 3407-169-68-1522;

# 2º
# Teste MLP - 2021-02-20 10:40:45.977257 - dataset_100_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 12, "activation": "relu"}, {"qtd_neurons": 8, "activation": "elu"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.1444; accuracy_model(%): 94.6237; accuracy_detection(%): 95.4704; rmse: 0.0701; precision: 0.9434; recall: 0.9516; f1-score: 0.9473; confusion_matrix: 3432-144-90-1500;

# 3º
# Teste MLP - 2021-02-20 10:39:52.312337 - dataset_100_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 12, "activation": "relu"}, {"qtd_neurons": 8, "activation": "relu"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.1409; accuracy_model(%): 94.4086; accuracy_detection(%): 94.8509; rmse: 0.0775; precision: 0.9338; recall: 0.9485; f1-score: 0.9406; confusion_matrix: 3392-184-82-1508;

















# METRICAS - Acurracy, Precision, Recall e F1 Score
https://medium.com/@vilsonrodrigues/machine-learning-o-que-s%C3%A3o-acurracy-precision-recall-e-f1-score-f16762f165b0

# BATCH SIZE
Neste experimento, vamos investigar o efeito do tamanho do lote (Batch Size) na dinâmica de treinamento. Tamanho do lote (Batch Size) é um termo usado em aprendizado de máquina e refere-se ao número de exemplos de treinamento usados em uma iteração. O Batch Size pode ser uma das três opções:

batch mode: onde o tamanho do lote é igual ao conjunto de dados total, tornando os valores de iteração e épocas equivalentes.
mini-batch mode: onde o tamanho do lote é maior que um, mas menor que o tamanho total do conjunto de dados. Geralmente, um número que pode ser dividido no tamanho total do conjunto de dados.
stochastic mode: onde o tamanho do lote é igual a um. Portanto, o gradiente e os parâmetros da rede neural são atualizados após cada amostra.

http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/

# 5.2  MODELO LSTM
Esse capítulo apresenta o treinamento e resultado das diferentes configurações realizadas com o modelo LSTM (Long Short Term Memory).

MODELO LSTM
A camada de Dropout serve para mitigar problemas de sobreajuste
Uma camada LSTM antes de cada camada LSTM subsequente deve retornar a sequência
O que pode ser feito definindo o parâmetro "return_sequences = True" na(s) camada(s) anterior(es)
