


# METRICAS
loss; accuracy_model(%); accuracy_detection(%); rmse; precision; recall; f1-score; confusion_matrix;



# MODELO MLP - TESTES COM FUNÇÕES DE ATIVAÇÃO VARIADAS - 100 PALAVRAS

# 1º
# Teste MLP - 2021-02-20 10:38:58.807306 - dataset_100_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 12, "activation": "relu"}, {"qtd_neurons": 8, "activation": "tanh"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.1483; accuracy_model(%): 95.0538; accuracy_detection(%): 95.4123; rmse: 0.0696; precision: 0.9402; recall: 0.9550; f1-score: 0.9471; confusion_matrix: 3407-169-68-1522;

# 2º
# Teste MLP - 2021-02-20 10:40:45.977257 - dataset_100_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 12, "activation": "relu"}, {"qtd_neurons": 8, "activation": "elu"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.1444; accuracy_model(%): 94.6237; accuracy_detection(%): 95.4704; rmse: 0.0701; precision: 0.9434; recall: 0.9516; f1-score: 0.9473; confusion_matrix: 3432-144-90-1500;

# 3º
# Teste MLP - 2021-02-20 10:39:52.312337 - dataset_100_palavras.csv
{"epochs": 50, "batch_size": 10, "layers": [{"qtd_neurons": 12, "activation": "relu"}, {"qtd_neurons": 8, "activation": "relu"}, {"qtd_neurons": 1, "activation": "sigmoid"}]}
métricas: loss: 0.1409; accuracy_model(%): 94.4086; accuracy_detection(%): 94.8509; rmse: 0.0775; precision: 0.9338; recall: 0.9485; f1-score: 0.9406; confusion_matrix: 3392-184-82-1508;

















# METRICAS - Acurracy, Precision, Recall e F1 Score
https://medium.com/@vilsonrodrigues/machine-learning-o-que-s%C3%A3o-acurracy-precision-recall-e-f1-score-f16762f165b0

# BATCH SIZE
Neste experimento, vamos investigar o efeito do tamanho do lote (Batch Size) na dinâmica de treinamento. Tamanho do lote (Batch Size) é um termo usado em aprendizado de máquina e refere-se ao número de exemplos de treinamento usados em uma iteração. O Batch Size pode ser uma das três opções:

batch mode: onde o tamanho do lote é igual ao conjunto de dados total, tornando os valores de iteração e épocas equivalentes.
mini-batch mode: onde o tamanho do lote é maior que um, mas menor que o tamanho total do conjunto de dados. Geralmente, um número que pode ser dividido no tamanho total do conjunto de dados.
stochastic mode: onde o tamanho do lote é igual a um. Portanto, o gradiente e os parâmetros da rede neural são atualizados após cada amostra.

http://deeplearningbook.com.br/o-efeito-do-batch-size-no-treinamento-de-redes-neurais-artificiais/

# 5.2  MODELO LSTM
Esse capítulo apresenta o treinamento e resultado das diferentes configurações realizadas com o modelo LSTM (Long Short Term Memory).

MODELO LSTM
A camada de Dropout serve para mitigar problemas de sobreajuste
Uma camada LSTM antes de cada camada LSTM subsequente deve retornar a sequência
O que pode ser feito definindo o parâmetro "return_sequences = True" na(s) camada(s) anterior(es)
